# Configuration for training medium THONK model
# Better parameter distribution for text generation

# Model configuration
model_name_or_path: null  # Start from scratch
hidden_size: 768  # 3x larger hidden dimension
num_heads: 12  # More attention heads
H_layers: 4  # More hierarchical layers
L_layers: 4  # More local layers
H_cycles: 2  # More reasoning cycles
L_cycles: 2  # More local cycles
max_position_embeddings: 512
use_act: true  # ACT enabled - our key innovation!
halt_max_steps: 8  # Up to 8 thinking steps per token

# Data configuration
dataset_name: "tatsu-lab/alpaca"
max_train_samples: null  # Use all 52,002 samples from Alpaca dataset
max_eval_samples: 500  # Increase eval samples for better validation
block_size: 512
preprocessing_num_workers: 4

# Training configuration
output_dir: "./outputs/thonk-medium"
overwrite_output_dir: true
do_train: true
do_eval: true
eval_strategy: "steps"
eval_steps: 500  # Evaluate every 500 steps

# Training hyperparameters
num_train_epochs: 3  # Train for 3 epochs
per_device_train_batch_size: 2  # Smaller batch due to larger model
per_device_eval_batch_size: 2
gradient_accumulation_steps: 8  # Effective batch size = 16
learning_rate: 0.0003  # Lower learning rate for larger model
warmup_steps: 1000  # More warmup for larger model
weight_decay: 0.1

# Optimizer settings
adam_beta1: 0.9
adam_beta2: 0.95  # As per LLaMA
adam_epsilon: 0.00000001
max_grad_norm: 1.0

# Logging
logging_steps: 50  # Log every 50 steps
logging_first_step: true
save_strategy: "steps"
save_steps: 1000  # Save checkpoint every 1000 steps
save_total_limit: 3
load_best_model_at_end: true
metric_for_best_model: "eval_loss"

# Performance
fp16: false  # Set to true if your GPU supports it
bf16: false  # Set to true if your GPU supports it (better than fp16)
dataloader_num_workers: 4

# Misc
seed: 42
report_to: "none"  # Change to "wandb" if you want to use Weights & Biases
