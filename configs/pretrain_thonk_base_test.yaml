# THONK Base Model Pretraining - Quick Test Configuration
# Smaller version for testing the pretraining pipeline
# Can run on a single GPU with limited resources

# Model configuration (same architecture, for testing)
model_name_or_path: null
vocab_size: 50257
hidden_size: 1024
num_heads: 16
H_layers: 6
L_layers: 6
H_cycles: 2
L_cycles: 2
max_position_embeddings: 512  # Shorter context for testing
use_act: true
halt_max_steps: 8  # Fewer steps for testing

# Dataset configuration - Using more stable, simpler dataset
datasets:
  # TinyStories is great for initial training - consistent, simple patterns
  - name: "roneneldan/TinyStories"
    split: "train"
    streaming: true
    weight: 1.0
    description: "Simple coherent stories for stable training"

  

# Training configuration
output_dir: "./outputs/thonk-base-pretrain-test"
overwrite_output_dir: true
do_train: true
do_eval: true
eval_strategy: "steps"
eval_steps: 250  # Less frequent eval to save time
save_strategy: "steps"
save_steps: 1000
save_total_limit: 2

# Limited training for testing
max_steps: 10000  # Reasonable for limited samples
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 4  # Effective batch size = 8 (better for stability)
gradient_checkpointing: false  # Not supported by THONK model yet

# Learning rate schedule
learning_rate: 0.0001  # 1e-4 - conservative for stability
lr_scheduler_type: "cosine"  # Simple cosine schedule
warmup_steps: 500  # Longer warmup for stability
weight_decay: 0.05  # Slightly increased for regularization
adam_beta1: 0.9
adam_beta2: 0.98  # Increased for more stable momentum
adam_epsilon: 0.00000001  # 1e-8
max_grad_norm: 0.5  # Reduced for more stable gradients

# Data processing
block_size: 512  # Shorter sequences for testing
preprocessing_num_workers: 4
dataloader_num_workers: 2
stream_buffer_size: 2000  # Increased for better data variety
shuffle_buffer_size: 10000  # Increased to avoid data repetition
eval_dataset_size: 500  # Small eval set
max_train_samples: 100000  # Limit training samples for testing (100k samples)

# Tokenizer
tokenizer_name: "gpt2"
use_fast_tokenizer: true

# Performance
fp16: false  # Set to true if your GPU supports it
bf16: false  # Set to true for A100/H100 GPUs
tf32: false  # Set to true only for Ampere GPUs (RTX 30xx, A100)
optim: "adamw_torch"

# Logging
logging_steps: 10
logging_first_step: true
report_to: "wandb"  # Re-enable wandb to track recovery

# Misc
seed: 42
dataloader_drop_last: true
remove_unused_columns: false
